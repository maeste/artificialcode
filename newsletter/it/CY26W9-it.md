In questo numero della newsletter esprimo un paio di opinioni forti. La prima √® essenzialmente contenuta in uno degli action items dell'ultima sezione:

> "Quando leggi di licenziamenti a causa dell'AI, cerca sempre il secondo comunicato: il contesto business spesso racconta una storia molto diversa"

Ovviamente mi riferisco ai recenti 4.000 licenziamenti di Block e all'AI washing che si √® osservato intorno alla notizia e agli annunci della stessa.

La seconda opinione, che viene soprattutto dal mio uso quotidiano dell'AI assisted coding, √® che Claude Code sia ad oggi la scelta pi√π naturale in quel campo, nonostante Codex ed altri avanzino nel numero di utilizzatori, restano a varie lunghezze. A questo aggiungo che le novit√† che arrivano da parte di Anthropic sembrano sempre molto ben ragionate per migliorare l'esperienza e le performance degli sviluppatori. Secondo me il motivo sta nel fatto che anche per gli sviluppi interni di Anthropic, Claude Code sia il principale contributor.

Prima di lasciarvi alla lettura delle notizie e delle mie analisi di cosa √® successo in settimana, fatemi dire cosa √® successo, sta per succedere o succeder√† nella mia agenda pubblica, per chi volesse seguire i miei interventi o volesse incontrarmi di persona (adoro scambiare opinioni con chiunque abbia voglia di farlo):

* [Podcast](https://risorseartificiali.com) con Alessio e Paolo:
  * Il 12 Marzo saremo al JUG di Milano per registrare la nostra prima puntata live. [Non mancate](https://www.eventbrite.com/e/risorse-artificiali-appunti-e-spunti-dal-mondo-dellai-tickets-1983617212480?aff=oddtdtcreator)
  * Sabato √® uscita una puntata in cui torniamo a parlare diffusamente di AI coding, agenti, e tante novit√†
  * Stiamo lavorando ad altre interviste e puntate con ospiti molto interessanti
* Da solo:
  * Mi hanno intervistato di nuovo al podcast opensource. Stavolta parlo di agenti, AI, AGI. [Uscita qui il 26](https://open.spotify.com/show/3EAhXkBUmHE1a8vFTH84Yg?si=bacd744b0f9c4a55). Ascoltatela e fatemi avere i vostri commenti
  * Il 24 Marzo sar√≤ al Voxxed Day a Zurigo. Io e Alessio [presentiamo un talk sull'AI assisted coding](https://vdz26.voxxeddays.ch/talk/?id=8057)
  * Il 25 Marzo sar√≤ speaker in questo meetup a Milano sul [Vibe Coding e Agentic Engineering](https://www.eventbrite.it/e/biglietti-meetup-13-vibe-coding-1983538213191?aff=ebdssbcategorybrowse)
  * Il 30 Maggio avr√≤ l'onore di essere uno dei PyCon Italia [speakers](https://2026.pycon.it/en/speakers)

Ma partiamo dall'AI research, perch√© anche questa settimana ci sono novit√† rilevanti sul fronte dei modelli.

---

## üî¨ Novit√† e ricerca nei modelli AI

### I Takeaways per gli AI Engineers

- **Takeaway 1:** La competizione AI si √® spostata dall'asse quantit√†-rilasci verso l'innovazione architettonica: DeepSeek V4 con Engram e Qwen3.5 con MoE ottimizzato mostrano che la frontier si sposta anche attraverso efficienza, non solo potenza bruta
- **Takeaway 2:** Google consolida la strategia multifront: dopo Gemini 3.x sugli LLM, Nano Banana 2 copre la generazione immagini con capacit√† native di riconoscimento e coerenza ‚Äî non pi√π modelli separati ma ecosistema integrato
- **Takeaway 3:** La voce come interfaccia naturale con i modelli √® una tendenza in accelerazione: strumenti come Wispr Flow segnalano un cambio di paradigma nell'interazione uomo-macchina che va oltre la semplice trascrizione

- **Action Items:**
  - Testa Nano Banana 2 su image generation con focus su testo e coerenza
  - Monitora il rilascio di DeepSeek V4 e l'architettura Engram

### Cosa succede questa settimana?

Una settimana di relativa calma nel mondo dei modelli...o quasi. Certo non ci sono 3 SOTA o 5 rilasci di nuovi modelli cinesi come nelle ultime settimane, ma ci sono almeno un paio di cose molto rilevanti ed altre che comunque confermano il panorama dell'AI sempre in grande fermento per quanto concerne l'incremento di performance (in termini di qualit√† e potenza) dei modelli di linguaggio e non solo. Partiamo dall'annuncio principale della settimana che √® l'arrivo sul mercato di Nano Banana 2, il nuovo modello di generazione di immagini di Google. Chiaramente √® parte della strategia di Google di portare avanti la loro offerta su tutti i fronti, cos√¨ dopo l'annuncio di Gemini 3.1 di settimana scorsa sul fronte degli LLM, ecco arrivare il modello stable diffusion (anche se forse definirlo "solo" stable diffusion √® limitativo). Cosa c'√® di nuovo? Tanto: ovviamente ottima qualit√†, soprattutto nelle figure umane e nel testo, grande coerenza tra una immagine e l'altra e capacit√† di editing dell'immagine. Ma non solo, nativa capacit√† di riconoscere le immagini generate e quella che i creatori definiscono "una grande conoscenza del mondo" per generare ambientazioni realistiche a partire da semplici prompt. La seconda notizia da sottolineare √® il rilascio da parte di Alibaba della nuova famiglia di modelli Qwen 3.5 (di cui vi ho gi√† accennato settimana scorsa, ma che merita un approfondimento). Come sempre una famiglia di modelli, non solo uno con benchmark notevoli su tutti i fronti. Insomma la big tech pi√π big della Cina non sta certo a guardare n√© gli US n√© la concorrenza interna che viene da startup come Moonshot, Z.ai o Deepseek. Proprio parlando di Deepseek, ci sono insistenti rumors di un imminente rilascio di Deepseek V4. Al di l√† di polemiche su presunte distillazioni usando i modelli SOTA americani (onestamente un po' sterili da chi ha usato dati e testi protetti da diritto d'autore per trainare i propri modelli), quello che voglio sottolineare da un punto di vista tecnico √® che si tratterebbe del primo modello ad usare una architettura Engram. Ci vorrebbe un articolo intero per discutere come Engram riduce la complessit√† quadratica della sparse attention e quindi l'uso di VRAM per la KV cache ed √® oltre lo scopo di questa newsletter. Per√≤ √® l'ennesima conferma di quanto Deepseek stia puntando sull'innovazione pi√π che sulla forza bruta.

Chiudiamo con una nota su una tendenza trasversale: si cominciano a vedere sempre pi√π frequentemente proposte di interazione vocale con il PC e con i modelli ‚Äî Wispr Flow ne √® un esempio concreto. Credo che sia una tendenza significativa, solida e molto interessante.

### I link della settimana

- [Nano Banana 2](https://blog.google/innovation-and-ai/technology/ai/nano-banana-2/) ‚Äî Il nuovo modello di generazione immagini di Google: alta qualit√† su figure umane e testo, coerenza multi-immagine e "conoscenza del mondo" per ambientazioni realistiche.
- [DeepSeek V4: Rumors vs Reality for the Next Big Coding Model](https://blog.kilo.ai/p/deepseek-v4-rumors-vs-reality-for) ‚Äî Analisi dei rumors su DeepSeek V4: architettura Engram, contesto 1M+ token e prezzi ~$0.27/M token, in un mercato gi√† molto competitivo.
- [Anthropic: Detecting and Preventing Distillation Attacks](https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks) ‚Äî Anthropic rivela campagne industriali di distillazione illecita su Claude da parte di DeepSeek, Moonshot e MiniMax tramite circa 24.000 account fraudolenti.
- [Wispr Flow](https://wisprflow.ai/) ‚Äî App voice-to-text AI per qualsiasi app e dispositivo, con auto-edit, supporto 100+ lingue e accesso gratuito illimitato durante il lancio Android.
- [Qwen3.5: Towards Native Multimodal Agents](https://qwen.ai/blog?id=qwen3.5) ‚Äî Alibaba rilascia Qwen3.5, famiglia MoE con 397B parametri (17B attivi), contesto 256K, 201 lingue, 19√ó pi√π veloce di Qwen3-Max, licenza Apache 2.0.

---

## ü§ñ Agentic AI

### I Takeaways per gli AI Engineers

- **Takeaway 1:** Guardrail architetturali, non solo nel prompt: la compressione del contesto pu√≤ far perdere istruzioni critiche ‚Äî serve sicurezza infrastrutturale indipendente dall'LLM
- **Takeaway 2:** Autonomia e capacit√† decisionale degli agenti sono la variabile critica per il successo in produzione ‚Äî misurarle √® prioritario, non opzionale
- **Takeaway 3:** Agenti su UI umane: come i robot umanoidi, l'inefficienza dell'interfaccia √® accettabile perch√© l'infrastruttura esiste gi√†

- **Action Items:**
  - Verifica che i tuoi guardrail agentici siano architetturali, non solo nel prompt
  - Leggi la ricerca Anthropic sull'autonomia degli agenti

### Cosa succede questa settimana?

Partiamo da un post su X che ha fatto molto parlare di s√©: quello di Summer Yue. Per chi non lo sapesse, lei √® responsabile di Safety & Alignment nel laboratorio di Superintelligence di Meta. Il post racconta di come si sia lasciata sfuggire di mano OpenClaw che stava per cancellare tutta la sua posta elettronica (pare che sia riuscita a sventare il disastro). Al di l√† delle considerazioni sul suo ruolo o su OpenClaw che lasciato troppo libero pu√≤ fare disastri, a me piace sottolineare che lei aveva dato una istruzione nel system prompt di non farlo, ma (pare per compressione del contesto) √® andata persa. Alla fine io dico brava ad averlo postato invece di tenerselo per s√©, speriamo che serva a far capire a tutti (e al mondo enterprise) che servono guardrail forti e fuori dal controllo degli LLM se e quando si fanno fare cose potenzialmente rischiose.

Gli altri due link ci parlano invece di agenti con sempre maggiori capacit√†, capaci di emulare l'uso umano del computer e con una indipendenza e capacit√† decisionale in crescita. Vi invito a leggere la ricerca di Anthropic perch√© appunto questa indipendenza e capacit√† decisionale sono le chiavi del successo o insuccesso degli agenti e di una economia degli agenti.

Il nuovo agente di Perplexity invece l'ho citato per mostrarvi che, esattamente come accade per la robotica con form factor umanoide, a volte sia pi√π conveniente fare in modo che gli agenti usino interfacce disegnate per gli umani, nettamente pi√π inefficienti di quelle fatte per interazione tra macchine. Perch√©? Semplicemente perch√© quelle UI ci sono gi√†.

### I link della settimana

- [Anthropic Research: Measuring AI Agent Autonomy in Practice](https://x.com/anthropicai/status/2024210053369385192) ‚Äî Framework Anthropic per valutare indipendenza e capacit√† decisionale degli agenti AI in diversi scenari di deployment, nel contesto della sicurezza agenti.
- [Introducing Perplexity Computer](https://www.perplexity.ai/hub/blog/introducing-perplexity-computer) ‚Äî Lavoratore digitale general-purpose che unifica le capacit√† AI in un unico sistema, opera interfacce umane autonomamente e pu√≤ girare per ore o mesi.
- [Summer Yue su X ‚Äî OpenClaw inbox incident](https://x.com/summeryue0/status/2025774069124399363) ‚Äî La responsabile Safety di Meta Superintelligence racconta come OpenClaw stesse per cancellare la sua inbox: 9.8 milioni di visualizzazioni, lezione pratica sui guardrail agentici.

---

## üíª AI Assisted Coding

### I Takeaways per gli AI Engineers

- **Takeaway 1:** La durata di un task autonomo portato a successo √® un KPI fondamentale per misurare la maturit√† degli agenti: i 25 ore di Codex e il 30% di PR autonome di Cursor sono i nuovi benchmark di riferimento
- **Takeaway 2:** L'auto-memory di Claude Code √® la forma pi√π pragmatica di apprendimento continuo verticale disponibile oggi: imperfetta ma concettualmente potente ‚Äî trasforma ogni sessione in esperienza persistente
- **Takeaway 3:** La sicurezza del codice generato da AI sta diventando una specializzazione a s√©: agenti come Claude Code Security Research non sono opzionali ma infrastruttura necessaria in un mondo dove il codice √® sempre pi√π generato

- **Action Items:**
  - Configura e sperimenta Claude Code auto-memory nel tuo progetto principale
  - Leggi l'articolo di Thariq su come modellare l'action space degli agenti

### Cosa succede questa settimana?

Tante notizie in quella che √® la categoria pi√π in fermento degli ultimi mesi, quella sull'AI assisted coding. Partiamo dall'evoluzione degli agenti autonomi in casa Cursor ed OpenAI. I primi dichiarano che circa il 30% delle loro PR arrivano da agenti autonomi, con un intervento umano minimo. I secondi riportano un impressionante task completato da Codex in circa 25 ore di lavoro: come ho gi√† detto tante volte, la lunghezza di un task autonomo portato a successo √® uno dei parametri fondamentali per valutare l'evoluzione degli agenti.

Per quanto riguarda le novit√†, Claude Code la fa da padrone in questo ambito, come √® sempre stato negli ultimi mesi. Il passo di novit√† in casa Anthropic per gli AI Engineer √® davvero difficile da tenere, ma le novit√† di questa settimana richiedono e meritano un approfondimento. Il concetto di auto-memory, in cui Claude capisce cosa di quanto accaduto in una sessione pu√≤ essere significativo come memoria di lungo termine, √® un concetto potente. Probabilmente ancora imperfetto, ma √® quanto pi√π si pu√≤ avvicinare ad una versione light di apprendimento continuo, almeno in un verticale. La sicurezza √® un tema fondamentale in un mondo in cui gran parte del codice √® generato, e quindi agenti specializzati come "Claude Code Security Research" diventeranno un fondamentale supporto per gli sviluppatori e anche un aiuto per chi si occupa primariamente di sicurezza.

Concludo menzionando l'interessante articolo di uno dei creatori di Claude Code che ci insegna come modellare i nostri flussi e le skills per ottimizzare l'interazione tra uomo e macchina, e anche l'articolo che prova a spiegare perch√© Claude √® ad oggi la scelta primaria tra gli agenti di coding per la stragrande maggioranza degli AI engineer (anche se Codex ha dichiarato un grosso incremento di utilizzatori, resta a notevole distanza).

### I link della settimana

- [Claude Code Security Research Preview](https://www.anthropic.com/news/claude-code-security) ‚Äî Anthropic lancia un'anteprima AI per identificare vulnerabilit√† nel codice come farebbero i ricercatori umani, con verifica multi-stadio, severity rating e approvazione obbligatoria degli sviluppatori.
- [GPT-5 Codex: 25-Hour Coding Sprint](https://developers.openai.com/cookbook/examples/codex/long_horizon_tasks) ‚Äî GPT-5.3-Codex completa autonomamente un progetto da 25 ore, generando ~30.000 righe di codice con memoria strutturata in markdown e verifica qualit√† ad ogni milestone.
- [Why Developers Keep Choosing Claude Over Every Other AI](https://www.bhusalmanish.com.np/blog/posts/why-claude-wins-coding.html) ‚Äî Analisi del perch√© Claude Code √® la scelta primaria: editing senza corrompere il codice circostante, lettura dei file corretti, task multi-step senza perdere il filo.
- [Claude Code Auto-Memory](https://x.com/trq212/status/2027109375765356723) ‚Äî Claude salva autonomamente contesto tra le sessioni: CLAUDE.md per le istruzioni dell'utente, MEMORY.md taccuino aggiornato autonomamente da Claude ad ogni sessione.
- [Lessons from Building Claude Code: Seeing like an Agent](https://x.com/trq212/status/2027463795355095314) ‚Äî Framework per modellare l'action space degli agenti: strumenti calibrati sulle capacit√† del modello, uso strategico di AskUserQuestion, scelta tool generici vs. specializzati.
- [Cursor Agent Computer Use](https://cursor.com/blog/agent-computer-use) ‚Äî Cursor lancia agenti cloud in VM isolate: pi√π del 30% delle PR interne ora create autonomamente da agenti, con monitoraggio via video e controllo remoto del desktop.

---

## üè¢ Business e societ√†

### I Takeaways per gli AI Engineers

- **Takeaway 1:** Il fenomeno dell'"AI washing" nei licenziamenti √® reale e va riconosciuto: quando un titolo sale del 20% annunciando tagli "a causa dell'AI" mentre il CEO ammette crescita disorganica post-COVID, i segnali d'allarme sono evidenti
- **Takeaway 2:** La posizione di Amodei sul Dipartimento della Difesa √® un caso raro di coerenza etica in campo AI: rinunciare a un contratto governativo per principio, in un settore dove la pressione economica √® enorme, merita attenzione come modello
- **Takeaway 3:** L'arrivo degli Apple smart glasses segnerebbe una svolta nell'adozione consumer dell'AI embodied: la UI Liquid Glass suggeriva da tempo che Apple si stesse preparando a questo form factor

- **Action Items:**
  - Quando leggi di licenziamenti "a causa dell'AI", cerca sempre il secondo comunicato: il contesto business spesso racconta una storia molto diversa
  - Monitora gli annunci Apple su smart glasses: potrebbero ridefinire il mercato consumer dell'AI wearable

### Cosa succede questa settimana?

Non si pu√≤ che partire dai licenziamenti di Block. Il 40% dei dipendenti, circa 4.000. Annunciato internamente e su X, dicendo che √® a causa dell'AI. Ed il titolo che balza di oltre +20% in un periodo nero per tutta la borsa statunitense. Se non √® AI washing questo... e in effetti lo stesso Dorsey ammette in un tweet successivo (a borse chiuse) che Block era cresciuta troppo in fretta ed in modo disorganico durante il periodo COVID. A cui andrebbero aggiunte anche considerazioni sul mercato dei pagamenti online (principale business di Block) in chiara difficolt√†.

Sia ben chiaro, non sono certo io la persona che nega gli impatti dell'AI su societ√† e lavoro: basta che leggiate la newsletter di due settimane fa per capire quanto sia preoccupato e ritenga fondamentale non farsi trovare impreparati da questa rivoluzione. Per√≤ dico anche che √® pi√π facile, comodo e redditizio in borsa, dare la colpa all'AI invece che a scelte imprenditoriali sbagliate.

Di tutt'altro tenore la notizia legata alla presa di posizione forte di Anthropic e del suo CEO Dario Amodei sull'uso di Claude da parte del dipartimento della difesa americano. Coerente e forte. A me √® piaciuto molto...certo sono certo che presto il dipartimento della difesa americana trover√† un altro fornitore, anzi lo ha gi√† trovato in OpenAI...speriamo rispettando i paletti che Altman dice di aver avuto come garanzia, anche se immagino siano un filo pi√π morbidi di quelli posti da Amodei per cui √® saltato il precedente accordo.

Chiudo con una notizia di tutt'altro tenore, ovvero i rumors sull'arrivo di smart glasses in casa Apple. Sinceramente √® da quando ho visto la UI tutta in trasparenza "Liquid Glass" che dico che urla "occhiali!!"

### I link della settimana

- [Statement from Dario Amodei on discussions with the Department of War](https://www.anthropic.com/news/statement-department-of-war) ‚Äî Amodei rifiuta le condizioni del Dipartimento della Difesa USA su sorveglianza di massa e armi autonome, mantenendo le salvaguardie etiche di Anthropic nonostante le pressioni.
- [Jack Dorsey annuncia i licenziamenti di Block](https://x.com/jack/status/2027129697092731343?s=20) ‚Äî Dorsey annuncia il licenziamento del 40% della forza lavoro di Block (~4.000 persone), attribuendolo all'AI e ai nuovi modelli di lavoro con team pi√π snelli.
- [Jack Dorsey ‚Äî secondo post](https://x.com/jack/status/2027290756793135253?s=20) ‚Äî In un post successivo (a borse chiuse), Dorsey ammette che Block era cresciuta troppo velocemente e in modo disorganico durante il periodo COVID.
- [Apple AI Smart Glasses](https://9to5mac.com/2026/02/21/apple-ai-smart-glasses-rumors-sounding-more-exciting/) ‚Äî Apple accelera lo sviluppo di occhiali smart AI con due fotocamere integrate, puntando a rivaleggiare con i Meta Ray-Bans nel mercato emergente dei wearable AI.
