*Questa settimana ha portato un'ondata di sviluppi significativi nel panorama dell'AI, e ho lavorato per estrarre i pattern pi√π significativi per la nostra community di ingegneri. Dalla relazione in evoluzione tra sviluppatori e assistenti AI per il coding alla ricerca sull'affidabilit√† dei modelli, questi trend rivelano sia opportunit√† immediate che cambiamenti strategici pi√π profondi nel modo in cui costruiamo e implementiamo i sistemi AI. Ho aggiunto la mia prospettiva su ogni sviluppo per aiutarti a capire non solo cosa sta succedendo, ma cosa √® veramente significativo sotto la superficie degli annunci.*   
*Per chi fosse interessato ad approfondire questi argomenti, l'episodio podcast di sabato (solo in italiano) su üì∫[Youtube](https://www.youtube.com/channel/UCYQgzIby7QHkXBonTWk-2Fg) e üéß [Spotify](https://open.spotify.com/show/16dTKEEtKkIzhr1JJNMmSF?si=900902f2dca8442e) ha coperto embedding, allucinazioni dei modelli, paper di ricerca, il mercato del lavoro e molto altro. Unisciti a noi, fai domande e lascia commenti.*

## üîß AI Assisted Coding

### Key Takeaways per AI Engineer

- **Rivoluzione della Comprensione Semantica:** Esistono server MCP usano vector embedding e grafici di dipendenze per comprendere il codice semanticamente, non solo sintatticamente  
- **Sviluppatore come Orchestratore:** Il ruolo si sposta dalla scrittura di codice alla gestione di agenti AI che gestiscono i dettagli implementativi  
- **Modelli di emdeggin per estendere i client :** L‚Äôuso di semplici modelli per embeddings possono aggiungere funzionalit√† semantiche lato client   
- **Action Items:**  
  - Sperimenta la ricerca semantica del codice usando gli embedding  
  - Costruisci o seleziona server MCP che supportino i tuoi pattern di coding pi√π comuni

### Cosa √® successo questa settimana?

Comincio dall'[articolo di Guillaume su In-browser Semantic Search con EmbeddingGemma](https://medium.com/google-cloud/in-browser-semantic-search-with-embeddinggemma-ce37c4a1ceaa) per due motivi. Primo, come ammette Guillaume, gran parte del codice che propone √® stato scritto con assistenza AI. Pur non essendo un esperto JavaScript, sapeva esattamente cosa voleva implementare e, da ingegnere esperto, il risultato √® stato eccellente. Secondo, si collega al rilascio di [Google del modello EmbeddingGemma](https://huggingface.co/google/embeddinggemma-300m), che fornisce uno strumento eccellente per programmatori che vogliono applicare una parte fondamentale dell'AI diversamente: per ricerche semantiche, ma anche per esplorare differenze semantiche o clustering. Un modello di embedding compatto pu√≤ diventare il coltellino svizzero del programmatore con idee, e ti invito a provarlo.

Il trend verso l'assistenza AI da command-line continua a rafforzarsi con [Cursor CLI](https://cursor.com/cli), confermando ulteriormente quello che ho menzionato ripetutamente nelle settimane recenti. Questo strumento CLI beta promette di accelerare lo sviluppo direttamente dal terminale, rappresentando un'evoluzione naturale mentre gli sviluppatori cercano di integrare l'assistenza AI nei loro workflow esistenti piuttosto che cambiare contesto verso interfacce web.

Particolarmente interessante √® l'uso del coding assistito da AI per ottimizzare gli strumenti AI stessi, come dimostra la [ricerca di Anthropic su Writing Effective Tools for Agents](https://www.anthropic.com/engineering/writing-tools-for-agents) in cui viene usato Claude stesso per scrivere tools MCP ottimizzati per Claude. √à una sorta di meta-ottimizzazione; dopotutto, chi meglio del client sa cosa dovrebbe fornire il server? I loro ingegneri hanno scoperto che gli agenti lavorano meglio con meno strumenti pi√π attentamente progettati piuttosto che wrappare ogni endpoint API. Gli strumenti ottimizzati per Claude hanno superato significativamente le versioni scritte dagli umani nei test interni, con agenti capaci di migliorare automaticamente i propri set di strumenti attraverso cicli di valutazione. Una dimostrazione ulteriore che scrivere tools ottimizzati per l‚ÄôAI √® diverso dallo scrivere API per essere umani, e sta diventando sempre pi√π importante.

Interessante il [lavoro di Faraaz Ahmad su come rendere pi√π efficienti gli agenti AI per il coding](https://faraazahmad.github.io/blog/blog/efficient-coding-agent/). La sua ricerca affronta inefficienze fondamentali negli assistenti AI per coding attuali: sono troppo spreconi (spendendo 60.000 token senza trovare tutte le funzioni rilevanti), non mantengono il contesto della codebase (azzerando la memoria ad ogni conversazione), hanno solo una comprensione superficiale (basandosi su ricerche testuali), e sono suscettibili al context rot. La soluzione di Ahmad combina vector embedding per la ricerca semantica con grafici di dipendenze memorizzati in Neo4j, ottenendo miglioramenti di efficienza drammatici. In un benchmark, la ricerca semantica ha ridotto l'uso di token da 47.000 a 5.000 token, una riduzione di quasi il 90%. Il suo [strumento GraphSense](https://github.com/faraazahmad/graphsense) open-source dimostra che applicando tecniche consolidate di computer science (vector embedding per la similarit√† semantica e database a grafo per il tracking delle dipendenze) possiamo rendere gli assistenti AI per coding molto pi√π pratici e cost-effective.

## ü§ñ Agentic AI

### Key Takeaways per AI Engineer

- **Convergenza dei Protocolli:** A2A per la comunicazione agent-to-agent e MCP per l'interazione agent-to-tool stanno diventando standard del settore  
- **Architettura delle Estensioni:** Il sistema di estensioni di A2A abilita funzionalit√† domain-specific senza frammentazione del protocollo  
- **Gestione di Memoria e Contesto:** Il REFRAG di Meta dimostra estensione del contesto 16x con decoding 31x pi√π veloce  
- **Action Items:**  
  - Valuta il protocollo A2A per architetture di sistemi multi-agent  
  - Implementa MCP per l'integrazione di tool nei progetti correnti

### Cosa √® successo questa settimana?

Comincio con le notizie su A2A, dove sono personalmente coinvolto insieme al mio team nello sviluppo della specifica, TCK e SDK Java. La convergenza osservata verso A2A, con progetti alternativi come [ACP che si unisce allo sforzo comune](https://lfaidata.foundation/communityblog/2025/08/29/acp-joins-forces-with-a2a-under-the-linux-foundations-lf-ai-data/), dimostra come A2A si stia affermando come il protocollo per la comunicazione tra agenti, complementando il gi√† affermato MCP per la comunicazione agent-to-tool. IBM Research ha lanciato ACP a marzo 2025 per la loro piattaforma BeeAI, ma quando A2A √® apparso un mese dopo, entrambi i team hanno immediatamente visto l'allineamento nei loro approcci e hanno iniziato ad esplorare l'unificazione. Oggi, ACP si fonde ufficialmente con A2A sotto la Linux Foundation, con Kate Blair di IBM research che si unisce al Technical Steering Committee insieme ai rappresentanti di Google, Microsoft, AWS, Cisco, Salesforce, ServiceNow e SAP.

Il [lancio ufficiale delle estensioni del protocollo A2A](https://developers.googleblog.com/en/a2a-extensions-empowering-custom-agent-functionality/) mostra quanto stiamo cercando di essere pragmatici: mentre la community ha chiaramente bisogno di uno standard chiaro e stabile, la velocit√† di evoluzione del settore richiede un modo per supportare estensioni ed esperimenti anche a livello di protocollo, con lifecycle diversi dalla specifica base ma ugualmente aperti e disponibility per la community. Le estensioni sono la risposta di A2A a questo dilemma. Chiunque pu√≤ definire, pubblicare e implementare un'estensione, ognuna identificata da un URI unico, rendendo l'ecosistema incredibilmente aperto e community-driven. Le estensioni coprono casi d'uso diversi: dall'estensione di tracciabilit√† che fornisce visibilit√† profonda per tracciare le interazioni degli agenti, alle estensioni domain-specific come quella di Twilio per la consapevolezza della latenza nella selezione del modello per agenti vocali, alle implementazioni di Identity Machines di handshake zero-trust per chiamate sicure agent-to-agent.

I chatbot diventano sempre pi√π agenti essi stessi, come dimostra la [nuova capacit√† di Claude di creare e modificare file](https://www.anthropic.com/news/create-files). Claude ora pu√≤ generare e modificare documenti, fogli di calcolo, presentazioni e PDF direttamente nell'app, permettendo agli utenti di trasformare prompt e dati in file scaricabili. Questa nuova capacit√† espande l'utilit√† di Claude oltre la semplice conversazione, rendendolo uno strumento pratico per la creazione di contenuti. Allo stesso modo, gli utenti vogliono sempre pi√π agenti, come mostrato dal successo di [Google AI Mode](https://searchengineland.com/google-ai-mode-may-become-the-default-google-search-experience-soon-461649), che Google dice diventer√† l'esperienza di ricerca predefinita "presto".

Il [lancio del MCP Registry](https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/) fornisce un modo standardizzato per distribuire e scoprire server MCP. Un progetto community-driven, permette alle organizzazioni di creare registry enterprise privati mantenendo la compatibilit√† attraverso schemi API condivisi e linee guida di moderazione. Il registry agisce come un "app store" per server MCP, facilitando la scoperta e l'implementazione di nuove integrazioni.

I [pensieri di Raindrop AI sulle Evals](https://www.raindrop.ai/blog/thoughts-on-evals) evidenziano che il monitoraggio in produzione rivela problemi reali che le valutazioni pre-deployment inevitabilmente perdono, specialmente quando i prodotti AI diventano pi√π imprevedibili e personalizzati. Le evals sono collezioni di casi di fallimento gi√† noti, ma gli agenti possono e spesso falliscono in modi che non producono codici di errore. I sistemi agentic richiedono approcci di valutazione pi√π sofisticati oltre i benchmark tradizionali.

Infine, il [paper REFRAG di Meta](https://www.marktechpost.com/2025/09/07/meta-superintelligence-labs-introduces-refrag-scaling-rag-with-16x-longer-contexts-and-31x-faster-decoding/) √® significativo non solo per RAG (il loro primo caso di studio) ma anche per la gestione di memoria e contesto negli agenti. Come ho menzionato ripetutamente nelle settimane recenti, questo √® dove si decide molta dell'efficacia degli agenti nel gestire contesto pi√π ampio in modo efficiente e in qualche modo intelligente. REFRAG estende le context window degli LLM di 16x e ottiene fino a 30.85x di accelerazione nel time-to-first-token senza compromettere l'accuratezza. Il meccanismo comprime i passaggi recuperati in embedding compatti, espandendo selettivamente solo quelli importanti. Usa un encoder leggero che divide i passaggi recuperati in chunk di dimensione fissa e comprime ognuno in un embedding denso, con una policy di reinforcement learning che supervisiona la compressione per identificare i chunk pi√π information-dense.

## üß† AI Models and Research

### Key Takeaways per AI Engineer

- **Allucinazioni come Problema di Incentivi:** La ricerca OpenAI mostra che i modelli allucinano perch√© l'addestramento premia l'indovinare rispetto all'incertezza  
- **Il Determinismo √® Ottenibile:** Thinking Machines Lab prova che il non-determinismo degli LLM deriva da problemi di batch invariance, non da limitazioni fondamentali  
- **Efficienza Attraverso la Sparsity:** Il modello 80B di Qwen3-Next attiva solo 3B parametri all'inferenza, ottenendo 10x throughput  
- **Action Items:**  
  - Progetta metriche di valutazione che premino l'espressione appropriata dell'incertezza  
  - Tenete un occhio sui modelli open source alternativi a quelli delle big tech

### Cosa √® successo questa settimana?

C'√® stata molta discussione sulla [ricerca sulle allucinazioni di OpenAI](https://openai.com/index/why-language-models-hallucinate/), che √® importante perch√© collega le allucinazioni a come i modelli vengono premiati durante l'addestramento. Se indovinare a caso √® pi√π premiante che dire "non lo so", il modello indoviner√†. Prendi l'esempio del compleanno di una persona: se indovinare correttamente vale 10 punti e sia essere sbagliati che dire "non lo so" valgono 0, ha senso indovinare a caso perch√© hai 1/365 possibilit√† di ottenere 10 punti. "Non lo so" garantisce zero. La soluzione √® concettualmente semplice: calibrare i premi durante l'addestramento basandosi sulla certezza, penalizzare pi√π pesantemente gli errori confidenti ma sbagliati, e dare credito parziale quando i modelli esitano o esprimono dubbio. Anche se concettualmente semplice, tuttavia richiede di rivedere come i dataset di training sono costruiti e quindi la sua implementazione potrebbe essere lunga e costosa per i modelli state of the art.

Forse ancora pi√π interessante e formale √® la [ricerca di Thinking Machines Lab](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/) che fornisce una spiegazione matematica del non-determinismo dei modelli. La loro ricerca, il primo output pubblico del nuovo lab di Mira Murati, identifica la vera causa del non-determinismo nei modelli: contrariamente alla credenza comune, non √® la natura non-associativa dell'aritmetica floating-point combinata con l'esecuzione concorrente su GPU, ma piuttosto la mancanza di batch invariance nei kernel di inferenza ampiamente utilizzati. Il problema principale √® che il carico (e quindi la dimensione del batch) varia in modo non-deterministico, causando risultati diversi anche con input identici. La ricerca fornisce implementazioni PyTorch di operazioni invarianti e dimostra Qwen3-8B che gira deterministicamente sotto vLLM.

Alibaba ha introdotto [Qwen3-Max-Preview](https://venturebeat.com/ai/qwen3-max-arrives-in-preview-with-1-trillion-parameters-blazing-fast), un modello con 1 trilione di parametri ora accessibile tramite Qwen Chat e Alibaba Cloud. I benchmark iniziali evidenziano miglioramenti nel seguire istruzioni, dialogo e performance di task agentic. Il modello supporta una context window di 262.144 token ed √® progettato per ragionamento complesso, coding, gestione di formati dati strutturati come JSON, e task creativi. [Qwen3-Next](https://qwen.ai/blog) rappresenta un avanzamento significativo nell'efficienza computazionale con la sua nuova architettura che presenta un design sparse Mixture-of-Experts con attenzione ibrida e predizione multi-token. Il suo modello base da 80B parametri attiva solo 3B parametri all'inferenza, abilitando un throughput 10x pi√π veloce per task long-context. I modelli di Alibaba si avvicinano sempre di pi√π a quelli delle big tech americane e al momento sono i migliori open source in circolazione; Il gap da colmare resta forse sulla dimensione del contesto che riescono a gestire

[GROK sta lavorando sul background thinking](https://x.com/nima_owji/status/1964725529015374171), con uno screenshot che mostra un toggle per la funzione che sembra permettere agli utenti di continuare a chattare mentre l'assistente pensa. Il [REER (REverse-Engineered Reasoning)](https://m-a-p.ai/REER_DeepWriter/) di ByteDance presenta un paradigma innovativo che deriva ragionamento step-by-step da risposte buone conosciute piuttosto che costruirlo in avanti attraverso RL o imitazione. Nel frattempo, [Seedream 4.0](https://www.scmp.com/tech/big-tech/article/3325058/bytedance-unveils-new-ai-image-model-rival-google-deepminds-nano-banana) di ByteDance afferma superiorit√† rispetto al "Nano Banana" di Google DeepMind nell'aderenza al prompt, allineamento ed estetica, combinando funzioni dalle versioni precedenti allo stesso costo di $30 per 1.000 generazioni, anche se queste affermazioni si basano sul benchmark interno MagicBench di ByteDance senza un report tecnico ufficiale.

## üîß Devices and Robotics

### Key Takeaways per AI Engineer

- **Breakthrough nella Coordinazione Neurale:** RoboBallet ottiene coordinazione multi-robot usando reti neurali grafiche  
- **Progresso nelle Brain-Computer Interface:** Il wearable di Alterego interpreta la comunicazione silenziosa attraverso micromovimenti facciali  
- **Accelerazione Edge AI:** Modelli di traduzione locali sui dispositivi mobili riducono la dipendenza dal cloud  
- **Action Items:**  
  - Esplora framework Edge AI per processing on-device  
  - Considera modelli edge multi-modali per applicazioni UI/UX

### Cosa √® successo questa settimana?

Guarda il [video di RoboBallet](https://x.com/GoogleDeepMind/status/1965040645103407572) per capire il livello di coordinazione multi-robot ottenuto dalle reti neurali. La ricerca di Google DeepMind presenta un approccio innovativo per pianificare movimenti coordinati tra pi√π robot usando reti neurali grafiche e reinforcement learning. RoboBallet affronta la sfida della produzione robotica moderna che richiede coordinazione senza collisioni di pi√π robot per completare numerosi task in spazi di lavoro condivisi e ricchi di ostacoli. Il sistema usa reinforcement learning per abilitare pianificazione scalabile di task e movimenti nella robotica multi-robot, combinando intelligentemente le capacit√† delle reti neurali grafiche per comprendere relazioni spaziali con algoritmi di reinforcement learning per ottimizzare le traiettorie.

Il [wearable di Alterego](https://alterego.ai/) sembra telepatico ma in realt√† sfrutta micromovimenti facciali e della lingua: devi parlare senza fare suoni, muovendo la lingua perch√© funzioni. Tuttavia, sembra fantascienza e certamente ha applicazioni per chi non pu√≤ usare le mani a causa di altri task o disabilit√†. La startup di Boston e spinoff del MIT Media Lab ha introdotto questo wearable "quasi-telepatico" che usa AI per rilevare movimenti sottili e interpretare la comunicazione silenziosa di un utente "alla velocit√† del pensiero" senza parlare ad alta voce.

Gli [AirPods con Apple Intelligence](https://x.com/GoogleDeepMind/status/1965040645103407572) su iPhone locale e i [modelli edge gallery di Google](https://x.com/GoogleDeepMind/status/1965040645103407572) che aggiungono anche audio sottolineano come piccoli modelli sui dispositivi mobili possano avere un futuro per le traduzioni. I nuovi AirPods Pro 3 includono traduzione dal vivo abilitata da Apple Intelligence in locale su iPhone.   
La AI Edge Gallery di Google ha aggiunto modalit√† audio attraverso Gemma 3n, il loro primo modello multimodale per dispositivi edge, con la nuova funzione "Audio Scribe" che permette trascrizione e traduzione completamente offline. Il mio forte sospetto √® che quanto stiamo vedendo su AI Edge Gallery sia la base di quanto stanno [preparando per i dispositivi home](https://blog.google/products/google-nest/gemini-for-home/) che potrebbero avvalersi di modelli locali per i compiti pi√π semplici relativi alla voce e delegare a Gemini i compiti pi√π complessi. Forse non al primo rilascio, ma a tendere potrebbe essere questa la strada.  
E probabilmente in generale per eseguire task specifici o funzioni di interfaccia utente, delegando ragionamento o task complessi ai modelli cloud (LLM). Ma gi√† fare la prima parte di UX in linguaggio naturale o anche UX audio ridurrebbe notevolmente il costo di qualsiasi chiamata API LLM remota. Questa mi sembra la direzione pi√π promettente per gli SLM.

## üè¢ Business and Big Tech

### Key Takeaways per AI Engineer

- **Emergono Tensioni nelle Partnership:** Microsoft diversifica da OpenAI integrando Claude in Office 365  
- **Surge di Investimenti in Physical AI:** Robotica e sistemi autonomi attirano round di finanziamento massicci  
- **Transizione For-Profit:** I cambiamenti strutturali di OpenAI segnalano la maturazione del settore  
- **Action Items:**  
  - Monitora strategie multi-modello per deployment enterprise  
  - Traccia investimenti in physical AI per opportunit√† emergenti

### Cosa √® successo questa settimana?

Le tensioni tra Microsoft e OpenAI dovrebbero essere enfatizzate, insieme al fatto che l'[accordo che stanno raggiungendo dovrebbe liberare OpenAI per diventare una compagnia for-profit](https://www.bloomberg.com/news/articles/2025-01-08/microsoft-openai-truce-clears-hurdle-in-path-to-for-profit-conversion). Microsoft e OpenAI hanno raggiunto un accordo per estendere la loro partnership, potenzialmente facilitando il percorso di OpenAI verso cambiamenti in una corporation for-profit. L'accordo rappresenta una vittoria per OpenAI, che aveva bisogno dell'approvazione di Microsoft per il suo piano non-profit prima che potesse essere formalmente condiviso con i regolatori statali. Nel frattempo, [Microsoft sta riducendo la dipendenza da OpenAI integrando Claude](https://techcrunch.com/2025/09/09/microsoft-to-lessen-reliance-on-openai-by-buying-ai-from-rival-anthropic/) nelle app Office 365, terminando la dipendenza esclusiva di Microsoft da OpenAI. Oltre a questo, come dicevamo la settimana scorsa, sta sviluppando modelli proprietari di grandi dimensioni per affrancarsi da ChatGPT. OpenAI ha continuato a stressare la sua relazione con Microsoft annunciando prodotti competitivi, incluso un rivale AI-powered di LinkedIn e chip personalizzati costruiti da Broadcom.

Le altre due notizie enfatizzano come gli investimenti si stiano muovendo anche verso la cosiddetta physical AI, ovvero la robotica. [I robotaxi Zoox di Amazon](https://www.cnbc.com/2025/09/10/amazons-zoox-jumps-into-us-robotaxi-race-with-las-vegas-launch-.html) iniziano la loro offerta sulla Las Vegas Strip, offrendo corse gratuite da location selezionate e pianificando di espandersi pi√π ampiamente nella citt√† il mese prossimo. Zoox diventa la prima startup a offrire corse completamente autonome in veicoli progettati da zero per operazioni driverless. [Physical Intelligence si avvicina a una valutazione di $5B](https://www.theinformation.com/articles/physical-intelligence-nears-5b-valuation), con la startup AI robotica di San Francisco riportata vicina a chiudere un round di finanziamento massiccio che valuterebbe la compagnia a $5 miliardi, posizionandosi come leader nell'AI robotica focalizzata su sistemi che possono interagire fisicamente con il mondo reale.
