In this issue of the newsletter I'm sharing a couple of strong opinions. The first is essentially captured in one of the action items from the last section:

> "When you read about layoffs 'because of AI', always look for the follow-up statement: the business context often tells a very different story"

Obviously I'm referring to the recent 4,000 layoffs at Block and the AI washing observed around the news and the company's announcements.

The second opinion, which comes mainly from my daily use of AI assisted coding, is that Claude Code is today the most natural choice in that field ‚Äî even though Codex and others are growing their user base, they're still several lengths behind. I'd add that the updates coming from Anthropic always seem very well thought out to improve the developer experience and performance. In my view, the reason is that even for Anthropic's own internal development, Claude Code is the primary contributor.

Before I leave you to read the news and my analysis of what happened this week, let me tell you what has happened, is about to happen, or will happen on my public agenda ‚Äî for anyone who wants to follow my talks or meet me in person (I love exchanging opinions with anyone who's up for it):

* [Podcast](https://risorseartificiali.com) with Alessio and Paolo:
  * On March 12 we'll be at the JUG in Milan to record our first live episode. [Don't miss it](https://www.eventbrite.com/e/risorse-artificiali-appunti-e-spunti-dal-mondo-dellai-tickets-1983617212480?aff=oddtdtcreator)
  * Saturday's episode came out ‚Äî we're back to talking extensively about AI coding, agents, and lots of news
  * We're working on more interviews and episodes with very interesting guests
* Solo:
  * I was interviewed again on the opensource podcast. This time I talk about agents, AI, AGI. [Out here on the 26th](https://open.spotify.com/show/3EAhXkBUmHE1a8vFTH84Yg?si=bacd744b0f9c4a55). Listen and let me know what you think
  * On March 24 I'll be at Voxxed Day in Zurich. Alessio and I [are presenting a talk on AI assisted coding](https://vdz26.voxxeddays.ch/talk/?id=8057)
  * On March 25 I'll be a speaker at this meetup in Milan on [Vibe Coding and Agentic Engineering](https://www.eventbrite.it/e/biglietti-meetup-13-vibe-coding-1983538213191?aff=ebdssbcategorybrowse)
  * On May 30 I'll have the honor of being one of the PyCon Italia [speakers](https://2026.pycon.it/en/speakers)

But let's start with AI research, because this week too there's relevant news on the models front.

---

## üî¨ AI Models News & Research

### Takeaways for AI Engineers

- **Takeaway 1:** AI competition has shifted from the quantity-of-releases axis toward architectural innovation: DeepSeek V4 with Engram and Qwen3.5 with optimized MoE show that the frontier advances through efficiency, not just raw power
- **Takeaway 2:** Google consolidates its multi-front strategy: after Gemini 3.x on LLMs, Nano Banana 2 covers image generation with native recognition capabilities and coherence ‚Äî no longer separate models but an integrated ecosystem
- **Takeaway 3:** Voice as a natural interface with models is an accelerating trend: tools like Wispr Flow signal a paradigm shift in human-machine interaction that goes beyond simple transcription

- **Action Items:**
  - Test Nano Banana 2 for image generation with focus on text and coherence
  - Follow the DeepSeek V4 release and the Engram architecture

### What's happening this week?

A relatively quiet week in the world of models... or almost. Sure, there are no 3 SOTA releases or 5 new Chinese model drops like in recent weeks, but there are at least a couple of very relevant things, and others that nonetheless confirm the AI landscape is still in great ferment when it comes to performance improvements (in terms of quality and power) of language models and beyond. Let's start with the main announcement of the week: the arrival of Nano Banana 2, Google's new image generation model. Clearly it's part of Google's strategy to advance their offering on all fronts ‚Äî so after last week's Gemini 3.1 announcement on the LLM side, here comes the stable diffusion model (although perhaps calling it "just" stable diffusion is limiting). What's new? A lot: obviously great quality, especially on human figures and text, strong coherence between images and image editing capabilities. But not only that ‚Äî native ability to recognize generated images, and what the creators define as "a great knowledge of the world" to generate realistic settings from simple prompts. The second noteworthy news is the release by Alibaba of the new Qwen 3.5 model family (which I already mentioned last week, but which deserves a deeper look). As always, a family of models ‚Äî not just one ‚Äî with impressive benchmarks across the board. In short, China's biggest big tech is certainly not sitting still, neither watching the US nor the internal competition coming from startups like Moonshot, Z.ai, or DeepSeek. Speaking of DeepSeek, there are persistent rumors of an imminent DeepSeek V4 release. Beyond the controversy over alleged distillation using American SOTA models (honestly a bit hollow coming from those who used copyright-protected data and text to train their own models), what I want to highlight from a technical standpoint is that this would be the first model to use an Engram architecture. It would take a whole article to discuss how Engram reduces the quadratic complexity of sparse attention and therefore the VRAM usage for the KV cache ‚Äî it's beyond the scope of this newsletter. But it's yet another confirmation of how DeepSeek is betting on innovation rather than brute force.

Let's close with a note on a cross-cutting trend: voice interaction with PCs and models is becoming increasingly common ‚Äî Wispr Flow is a concrete example. I believe this is a significant, solid, and very interesting trend.

### This week's links

- [Nano Banana 2](https://blog.google/innovation-and-ai/technology/ai/nano-banana-2/) ‚Äî Google's new image generation model: high quality on human figures and text, multi-image coherence and "world knowledge" for realistic settings.
- [DeepSeek V4: Rumors vs Reality for the Next Big Coding Model](https://blog.kilo.ai/p/deepseek-v4-rumors-vs-reality-for) ‚Äî Analysis of the DeepSeek V4 rumors: Engram architecture, 1M+ token context and ~$0.27/M token pricing, in an already very competitive market.
- [Anthropic: Detecting and Preventing Distillation Attacks](https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks) ‚Äî Anthropic reveals industrial-scale illicit distillation campaigns on Claude by DeepSeek, Moonshot and MiniMax through approximately 24,000 fraudulent accounts.
- [Wispr Flow](https://wisprflow.ai/) ‚Äî AI voice-to-text app for any app and device, with auto-edit, 100+ language support and unlimited free access during the Android launch.
- [Qwen3.5: Towards Native Multimodal Agents](https://qwen.ai/blog?id=qwen3.5) ‚Äî Alibaba releases Qwen3.5, MoE family with 397B parameters (17B active), 256K context, 201 languages, 19√ó faster than Qwen3-Max, Apache 2.0 license.

---

## ü§ñ Agentic AI

### Takeaways for AI Engineers

- **Takeaway 1:** Architectural guardrails, not just in the prompt: context compression can cause critical instructions to be lost ‚Äî infrastructure-level security independent of the LLM is needed
- **Takeaway 2:** Agent autonomy and decision-making capability are the critical variable for production success ‚Äî measuring them is a priority, not optional
- **Takeaway 3:** Agents on human UIs: like humanoid robots, interface inefficiency is acceptable because the infrastructure already exists

- **Action Items:**
  - Make sure your agentic guardrails are architectural, not just in the prompt
  - Read the Anthropic research on agent autonomy

### What's happening this week?

Let's start with an X post that generated a lot of buzz: Summer Yue's. For those who don't know her, she's responsible for Safety & Alignment at Meta's Superintelligence lab. The post describes how she lost control of OpenClaw, which was about to delete her entire inbox (apparently she managed to avert the disaster). Beyond the considerations about her role or about OpenClaw running amok when given too much freedom, I like to highlight that she had given an instruction in the system prompt not to do it ‚Äî but (apparently due to context compression) that instruction was lost. In the end, kudos to her for posting it instead of keeping it to herself. Hopefully it will help everyone (and the enterprise world) understand that strong guardrails outside LLM control are essential when delegating potentially risky actions.

The other two links talk about agents with ever-greater capabilities, able to emulate human computer use with growing autonomy and decision-making capacity. I invite you to read the Anthropic research because this autonomy and decision-making capability are the keys to the success or failure of agents and of an agent economy.

I cited Perplexity's new agent to show you that ‚Äî just like in robotics with humanoid form factors ‚Äî it's sometimes more convenient to have agents use interfaces designed for humans, even if they're far less efficient than machine-to-machine interfaces. Why? Simply because those UIs already exist.

### This week's links

- [Anthropic Research: Measuring AI Agent Autonomy in Practice](https://x.com/anthropicai/status/2024210053369385192) ‚Äî Anthropic framework for evaluating independence and decision-making capability of AI agents in various deployment scenarios, in the context of agent safety.
- [Introducing Perplexity Computer](https://www.perplexity.ai/hub/blog/introducing-perplexity-computer) ‚Äî General-purpose digital worker that unifies AI capabilities in a single system, autonomously operates human interfaces and can run for hours or months.
- [Summer Yue on X ‚Äî OpenClaw inbox incident](https://x.com/summeryue0/status/2025774069124399363) ‚Äî Meta Superintelligence's Safety lead shares how OpenClaw nearly deleted her inbox: 9.8 million views, a practical lesson on agentic guardrails.

---

## üíª AI Assisted Coding

### Takeaways for AI Engineers

- **Takeaway 1:** The duration of a successfully completed autonomous task is a fundamental KPI for measuring agent maturity: Codex's 25 hours and Cursor's 30% autonomous PRs are the new reference benchmarks
- **Takeaway 2:** Claude Code's auto-memory is the most pragmatic form of vertical continuous learning available today: imperfect but conceptually powerful ‚Äî it turns every session into persistent experience
- **Takeaway 3:** Security of AI-generated code is becoming a specialization in its own right: agents like Claude Code Security Research are not optional but necessary infrastructure in a world where code is increasingly generated

- **Action Items:**
  - Set up and experiment with Claude Code auto-memory on your main project
  - Read Thariq's article on how to model the action space of agents

### What's happening this week?

Lots of news in what has been the most active category of recent months: AI assisted coding. Let's start with the evolution of autonomous agents at Cursor and OpenAI. The former declare that around 30% of their PRs now come from autonomous agents with minimal human intervention. The latter report an impressive task completed by Codex in around 25 hours of work: as I've said many times, the length of an autonomous task successfully completed is one of the fundamental parameters for evaluating the evolution of agents.

As for the news, Claude Code dominates this space, as it has for the past several months. The pace of innovation at Anthropic for AI Engineers is genuinely hard to keep up with, but this week's updates deserve a deeper look. The concept of auto-memory ‚Äî where Claude understands what happened during a session and what might be meaningful as long-term memory ‚Äî is a powerful concept. Probably still imperfect, but it's as close as we can get to a lightweight form of continuous learning, at least within a specific vertical. Security is a fundamental theme in a world where much of the code is generated, and so specialized agents like "Claude Code Security Research" will become essential support for developers and a helpful tool for those primarily focused on security.

I'll close by mentioning the interesting article from one of Claude Code's creators, who teaches us how to model our workflows and skills to optimize human-machine interaction ‚Äî and the article that attempts to explain why Claude is today the primary choice among coding agents for the vast majority of AI engineers (even though Codex has announced significant user growth, it remains a considerable distance behind).

### This week's links

- [Claude Code Security Research Preview](https://www.anthropic.com/news/claude-code-security) ‚Äî Anthropic launches an AI preview to identify code vulnerabilities the way human security researchers would, with multi-stage verification, severity rating and mandatory developer approval.
- [GPT-5 Codex: 25-Hour Coding Sprint](https://developers.openai.com/cookbook/examples/codex/long_horizon_tasks) ‚Äî GPT-5.3-Codex autonomously completes a 25-hour project, generating ~30,000 lines of code with structured markdown memory and quality verification at every milestone.
- [Why Developers Keep Choosing Claude Over Every Other AI](https://www.bhusalmanish.com.np/blog/posts/why-claude-wins-coding.html) ‚Äî Analysis of why Claude Code is the primary choice: editing without corrupting surrounding code, reading the right files before making changes, multi-step tasks without losing the thread.
- [Claude Code Auto-Memory](https://x.com/trq212/status/2027109375765356723) ‚Äî Claude autonomously saves context between sessions: CLAUDE.md for user instructions, MEMORY.md a notebook Claude updates on its own every session.
- [Lessons from Building Claude Code: Seeing like an Agent](https://x.com/trq212/status/2027463795355095314) ‚Äî Framework for modeling agent action space: tools calibrated to model capabilities, strategic use of AskUserQuestion, choice between generic vs. specialized tools.
- [Cursor Agent Computer Use](https://cursor.com/blog/agent-computer-use) ‚Äî Cursor launches cloud agents in isolated VMs: over 30% of internal PRs now created autonomously by agents, with video monitoring and remote desktop control.

---

## üè¢ Business & Society

### Takeaways for AI Engineers

- **Takeaway 1:** The "AI washing" phenomenon in layoffs is real and must be recognized: when a stock jumps 20% on announcements of cuts "because of AI" while the CEO admits post-COVID disorganized growth, the warning signs are clear
- **Takeaway 2:** Amodei's stance on the Department of Defense is a rare case of ethical consistency in AI: declining a government contract on principle, in an industry where economic pressure is enormous, deserves attention as a model
- **Takeaway 3:** The arrival of Apple smart glasses would mark a turning point in consumer adoption of embodied AI: the Liquid Glass UI has long suggested Apple was preparing for this form factor

- **Action Items:**
  - When you read about layoffs "because of AI", always look for the follow-up statement: the business context often tells a very different story
  - Follow Apple's smart glasses announcements: they could redefine the consumer AI wearable market

### What's happening this week?

You can't start anywhere but the Block layoffs. 40% of employees, about 4,000. Announced internally and on X, citing AI as the reason. And the stock jumps over +20% in a period that's been terrible for the entire US market. If that's not AI washing... and indeed Dorsey himself admits in a follow-up post (after market close) that Block had grown too fast and in a disorganized way during COVID. To which one should add considerations about the online payments market (Block's main business) which is clearly struggling.

Let me be clear: I'm certainly not someone who denies AI's impact on society and work ‚Äî just read the newsletter from two weeks ago to understand how concerned I am and how fundamentally important I think it is not to be caught unprepared by this revolution. But I also say that it's easier, more convenient, and more profitable on the stock market to blame AI rather than bad business decisions.

A very different kind of story is the strong stance taken by Anthropic and its CEO Dario Amodei on the use of Claude by the US Department of Defense. Consistent and principled. I liked it a lot... though I'm quite sure the Department of Defense will soon find another supplier ‚Äî in fact, it already has in OpenAI... hopefully respecting the guardrails that Altman claims he has guaranteed, even if I imagine they're a bit softer than those Amodei set, which is what caused the previous deal to fall through.

I'll close with a very different kind of news: the rumors about Apple smart glasses. Honestly, ever since I saw the fully transparent "Liquid Glass" UI, I've been saying it's screaming "glasses!!"

### This week's links

- [Statement from Dario Amodei on discussions with the Department of War](https://www.anthropic.com/news/statement-department-of-war) ‚Äî Amodei refuses the US Department of Defense's conditions on mass surveillance and autonomous weapons, maintaining Anthropic's ethical safeguards despite the pressure.
- [Jack Dorsey announces Block layoffs](https://x.com/jack/status/2027129697092731343?s=20) ‚Äî Dorsey announces the layoff of 40% of Block's workforce (~4,000 people), attributing it to AI and new work models with leaner teams.
- [Jack Dorsey ‚Äî follow-up post](https://x.com/jack/status/2027290756793135253?s=20) ‚Äî In a follow-up post (after market close), Dorsey admits that Block had grown too fast and in a disorganized way during the COVID period.
- [Apple AI Smart Glasses](https://9to5mac.com/2026/02/21/apple-ai-smart-glasses-rumors-sounding-more-exciting/) ‚Äî Apple accelerates development of AI smart glasses with two integrated cameras, aiming to compete with Meta Ray-Bans in the emerging AI wearable market.
