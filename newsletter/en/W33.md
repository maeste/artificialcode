Four months of weekly newsletters without missing a single week. It's been quite a journey for me while writing them, but especially while reading the material that's shaped my opinions and that I've tried to share with you for deeper exploration. I think it's time to make a stop on this journey to take stock of what I think about the topics we've covered, focusing on my opinions and diving deeper into the aspects I care about most. I'll return to the news next week.

This isn't your typical weekly news roundup. Instead, I'm taking a step back to reflect on the four major trends we've tracked these past months, sharing what's stuck with me most and what's actively influencing how we work in software today. This is something I plan to do roughly every quarter, giving both myself and you a chance to take a deep breath, reflect and hopefully spark some valuable discussions and exchanges of opinions.

If you want to dive deeper into any of these topics, we discussed them at length in our Saturday podcast (Italian only) on [Youtube](https://www.youtube.com/channel/UCYQgzIby7QHkXBonTWk-2Fg) and [Spotify](https://open.spotify.com/show/16dTKEEtKkIzhr1JJNMmSF?si=900902f2dca8442e).

## üõ†Ô∏è AI Assisted Coding: Beyond the Hype to Real Transformation

### My takeaways from last quarter

Vibe coding versus AI assisted coding. I can't start discussing AI's influence on software without acknowledging these are two fundamentally different beasts. Vibe coding, as originally defined by Andrei Karpathy, is that magical experience of generating an entire application from a well-crafted prompt and letting AI do most of the heavy lifting, ideally without further intervention. It's fascinating and incredibly appealing for newcomers to coding, but experienced developers know this only works for greenfield applications and specifically for prototypes.

When we're talking about complex applications with thousands or millions of lines of code and high quality standards, it's what I call AI assisted coding the real game changer. The AI becomes a partner to the experienced developer who understands architecture and requirements, knows how to make the right requests, adjust course when the coding agent veers off track, suggest corrections, write tests, and perform external verifications. This fundamental difference elevates AI assisted coding to an entirely different level.

I'm not talking about just intelligent code completion here. I mean a full coding agent experience like Cursor in Agent mode, or various CLIs like Claude Code or Gemini CLI. Only this way you achieve a complete AI assisted coding experience. CLI interfaces deliver the best results, even though initially it might feel strange and sort of a step backward to abandon your IDE, they keep you much more focused on partnering with AI giving it instructions, directions and corrections during the development cycle. It‚Äôs then pretty normal to switch to the IDE for doing your part, getting back to your AI partner in CLI when needed. My favorite remains Claude Code, closely followed by Gemini CLI. Regarding models, I'll admit that this week I tried ChatGPT-5 for writing code and the results are undeniably close to Claude's, though Claude remains my preference.

The famous 10x speed boost exists in specific cases, though perhaps not always. But the feeling of working at least 5x speed is constant. Not on day one, of course. Like everything, you need to learn, become savvy, know how to craft the right prompt, curate conversation context by choosing which files and docs the AI access in a specific moment, and organize an effective workflow. [Many research studies suggesting much smaller improvements](https://addyo.substack.com/p/the-reality-of-ai-assisted-software), should not surprise or confuse you, since they refer to 2024 and users primarily using code completion. Things have radically changed in 2025, particularly in recent months. I definitely confirm my feeling of an improvement factor between 5 and 10 times, in my daily personal experience.

The developer's role is transforming radically. Being able to read code and understand system architecture becomes more important than writing impeccably. One of the most positive side effects is the ability to be polyglot in languages. If you're a good architect and know how to use these tools, you're free to use the language you prefer or that's most suitable for the problem at hand.

The amount of code being developed with AI assistance today is incredible. Just keep an eye on GitHub to see it. My team and I often jokingly comment in private some pull requests saying "Cursor (or Claude) did a great job here." But if you want something concrete, check out this [repository](https://github.com/a2aproject/a2a-tck). Is it totally written by AI? No, but AI assisted coding has undoubtedly become my regular practice.

Is the developer's job disappearing? Will all code be written by AI? I don't think so. I believe that soon 95% of code will be written by AI, but that remaining 5% will be much more than 100% of the code written in December 2023\. What I mean is that AI assisted coding is unlocking an unexpected amount of code creation, solving problems for which writing code was too expensive or inconvenient. Moreover, the increased speed of code writing increases the need for good software architectures and excellent software engineers who can adapt to this new mode.

**Action items:**

* Start using CLI coding agents today.
* Master context curation.
* Focus on architecture over syntax.

## üß† Agents: What You Need to Know

### My takeaways from last quarter

2025 is undoubtedly the year of agents. Not for adoption, we'll need to wait a bit longer for that, but certainly for the ferment in the software world. Everyone has their agents: ChatGPT Agent, Manus, Spark, Google AI Mode, and coding agents are everywhere. Then there's the explosion of [MCP](https://modelcontextprotocol.io/docs/getting-started/intro) for adding tools to an LLM to make it an agent, or the advent of [Agent2Agent](https://a2a-protocol.org/v0.3.0/specification/) protocol for multi-agent communication.

But what exactly is an agent and what's a multi-agent or agentic system? There aren't widely accepted general definitions. Let me simplify as much as possible and give you mine. **An AI Agent is** an intelligent system, typically powered by a model, capable of perceiving system state, taking actions, verifying them, and memorizing them to evolve its capabilities over time. Let's unpack this.

* Perceiving and taking actions translates to accessing tools, typically via MCP, meaning the ability to call existing or custom-developed services to read state or data and take actions to change state.
* Self-verification happens through systems developed to assess LLMs, and in general, they are perfect to evaluate results in a nondeterministic environment. We call them Eval.
* Finally, on evolving capabilities there's no complete agreement yet, but certainly it requires both short and long-term memory.

**An agentic system consists** of multiple agents communicating with each other. But here's the spontaneous question: aren't agents themselves tools with internal intelligence? [Short answer: No](https://discuss.google.dev/t/agents-are-not-tools/192812). The linked article gives a formal definition, but let me simplify again.

* A tool is something that gives additional capabilities to a particular model, used directly by the model to accomplish a task. Something I manage directly, handling input, output, and deciding what to do with it.
* Another agent is something I delegate a task to, that I don't control directly and whose abstract capabilities I only know, whose execution is opaque to me, that performs tasks I can't satisfy even with the best tools.

**Let me use an analogy for this**: a tool is a wrench I can use for many things like fixing my bicycle, which I know how to do. But if the bathroom leaks, I call a plumber. The wrench isn't enough; I need different expertise. Another reason for delegation is the opacity or confidentiality of data used to generate a response, where I don't want to have more than . Finally, there can be economies of scale reasons for using an agent instead of a tool. Think of a print shop that prints for you: theoretically, I could get their tools and do it myself, but it's not convenient.
The last distinction I‚Äôd like to make is between **horizontal agents and vertical ones**. Horizontal agents like chatGPT agent, Manus and others are designed to be consumer-friendly, taking a large set of problems and solving them using generic tools and strategy (typically implementing computer usage). They typically take a user‚Äôs request and solve it at their best. Vertical agents are more specific, specifically trained to solve a very specific task with specialised tools, tailored knowledge, access to reserved data. They are typically orchestrated to compose different verticals and solve the entire problem. To stay in my analogy above, horizontals are the generalist manutentator or DYI passionate, while the verticals are a team of plumbers, electricians, and bricklayers, orchestrated by the foreman.

So after all these definitions, where are we? Agents are certainly the hot topic of recent months and definitely the most promising development area for seeing AI applied in enterprise settings. As AI Engineers, we need to start thinking of agents as the components that constitute and will constitute future AI architectures. But we must also keep in mind that the frameworks we're developing for building agents can't ignore the fundamental components I mentioned above and must facilitate their use by those who'll develop, deploy, monitor, and customize agents in enterprises:

* The LLM is the agent's brain and you can't do without it. Optimizing context and prompt is no longer just a data scientist's job and will be less so. Frameworks must help software engineers with this.
* Evals and tracing systems are an integral part of the system. In an inherently non-deterministic system, being able to evaluate behavior is more fundamental than ever.
* Tools and their protocols, MCP in this case, are important, but so is the design of tools and their APIs to be AI-friendly.
* Communication between agents requires protocols like A2A, which I'm betting on, but the ability to delegate and orchestrate multiple agents is also key.
* Memory, both short and long-term, is so undervalued yet so important.

**Action items:**

* Experiment with MCP tools.
* Experimenting with multi agent system and A2A protocol
* Design AI-friendly APIs.
* Implement proper eval ad tracing systems.

## ü§ñ Model Improvements: Breaking Through Impossible Walls

### My takeaways from last quarter

We could talk for hours about model progress alone. These months we've seen a continuous race between major players OpenAI, Anthropic, Google, and an explosion of open-source competitors. But the point I want to focus on, perhaps the most important for us in the software world, is how this rapid development isn't just about improving characteristics, but about how models learn to do new and increasingly sophisticated things we thought impossible just before.

As Dario Amodei says in a [recent interview](https://youtu.be/GcqQ1ebBqkc?list=TLGGndLmhjwsKA8xNjA4MjAyNQ), we've been accustomed to thinking there were insurmountable walls regarding these models' capabilities that we've gradually accepted being overcome: "they'll never make coherent speech" (pre-ChatGPT), "they'll never simulate a human voice," "they'll never use words to reason," "they'll never use tools," "they'll never know how to collaborate." In these few years and even more in recent months, we've seen all these walls crumble. Models become increasingly good at doing things we thought impossible.

Another fascinating interview is with [Demis Hassabis](https://youtu.be/-HzgcbRXUK8?list=TLGGrx-9q6U0a80xNjA4MjAyNQ) who shares many interesting insights. What struck me most is his clear definition of how neural network models are very good at descending an error gradient curve, managing to guess a structure and then mimic it, when complexity makes it impossible or very difficult for us to formalize, even though it appears obvious that a structure exists. This is why models have recently been able to do protein folding, why we're working on DNA structures with models, or mountain shapes, or hurricane progression. All natural elements where it's evident a structure exists, but what this structure is eludes us.

But how does this translate to what we see daily? Let's start with one criticism many have insisted on recently on social media about generated videos. Everyone focused on aspects of final video rendering, perfectly legitimate if you want to use these tools to produce a viral video or something professional. But what we shouldn't miss as software engineers is that those videos have learned the physics of liquids or light refraction in a way very close to reality. And they did it just by observing the provided examples. If you have any idea how to write the equation representing an ocean wave, assuming it's even possible, you understand how incredible this is. Not to mention world models that reproduce this physics in real time.

LLM models have also made incredible progress, especially in managing long contexts. A few months ago we talked about 64k tokens; now both Gemini and Claude comfortably reach 1 million tokens. This is very significant in practical implications like understanding enormous text bases, and more importantly for us, code. Looking at this from another perspective, it is incredible the progress necessary in attention mechanisms to achieve these results.

We cannot skip the small models progression and how their fine-tuning is becoming easier and more adopted to reach similar performance of larger models, at least in very specific tasks. This brings us back to multi-agent systems and vertical agents that may decide to use a highly specialized, cheap small model to solve specific tasks. Spcialization of models culd be an important them in next months, for sure for small models, but also large models are specializing their capabilities more and more. So we see Claude highly specialised in coding, O3 in creative writing, Gemini in maths and physics, and this specialization is even more pronounced if we look at different abilities like video generation or image processing. More specialization for models, could mean in the near future more space for multi-agent vertical agents orchestrated to solve problems which complexities span multiple capabilities. In the end, is how a squad of experts often does better than a single genius. Or if you prefer different areas of our brain. The AGI (Artificial General Intelligence) is probably coming in the agent space, more than in a single model: in fact GPT5, GROK4, Gemini DeepThink are already experimenting with their ‚Äúparrallel‚Äù thinking (which is multiple instance of the LLM used in an agentic way)

But there is more science fiction becoming reality soon. Current and near-future developments, as Hassabis also says in his interview, go toward embodiment, namely robotics. To take further steps in intelligence, he says, models need physical experience. Is the "ChatGPT moment" coming for robotics? I'm betting yes.

**Action items:**

* Leverage long context windows.
* Prepare for embodied AI.
* Stay current with model capabilities.
* Look forward for agentic AGI

## üíº Business and Society: Riding the Fourth Industrial Revolution

### My takeaways from last quarter

The impacts on society are already significant and will continue to be. Many call this moment the fourth industrial revolution, and like all industrial revolutions, we'll need to adapt to new ways of working, studying, and doing business. Especially if you are or deal with young people or the very young, AI will be for them what the internet was for us, or perhaps more, what printing was for our great-great-grandparents.

I often hear comparisons with the dot-com bubble. Whether this AI surge in markets is financial speculation, I have neither the expertise nor interest to say. What I can tell you is that the technology is here to stay, just as the dot-coms were a bubble, but if you're reading me it's because the technology from that bubble, the internet, is here and has changed society, work, study, and business considerably.

But let's close with us. What does this new way of doing business mean, where everything moves so rapidly? For this too, the role of software engineers must change, the mindset must change. If before we were used to developing software to solve a problem that remained rather stable over time, the adaptation required of us today is precisely having to solve problems that are true today but might be solved by model advancement tomorrow. So even more than before, we're required to be agile, with an extremely lean product philosophy, creating solutions that solve very specific problems and are sufficiently minimal to go to the the market right now. Making the API that performs task X that a model can't do today is what we must do. But it must be minimal and ready immediately, because if we take time to create that perfect API or framework but meanwhile the problem is already solved, we've just wasted work. Moreover the solution should be flexible, designed with AI in mind and able to adapt to new LLM capabilities coming. More and more our software needs to be designed for the internet of agents, not only for human users.
The ability to adapt, be fast, thanks also to the AI assisted coding, minimal and agile both in decisions and execution are today the most important characteristics for moving in this frenetic and constantly changing market. And my feeling is that we are still in the hype curve, far from the stability in the adoption or the technology if we look to the AI as a whole and not only for LLM (think about what was mentioned above about multi-agent AGI and AI embodiment).
So the future can be bright, exciting, but indeed challenging. And in rapid changing landscapes only the most adaptable creatures survive and thrive.

**Action items:**

* Adopt lean philosophy.
* Ship minimal viable solutions.
* Stay agile in execution.
