*This week brought a wave of significant developments across the AI landscape, and I've worked to extract the most meaningful patterns for our engineering community. From the evolving relationship between developers and AI coding assistants to research on model reliability, these trends reveal both immediate opportunities and deeper strategic shifts in how we build and deploy AI systems. I've added my perspective on each development to help you understand not just what's happening, but what's genuinely significant beneath the surface announcements.*   
*For those interested in diving deeper into these topics, Saturday's podcast episode (Italian only) on 📺[Youtube](https://www.youtube.com/channel/UCYQgzIby7QHkXBonTWk-2Fg) and 🎧 [Spotify](https://open.spotify.com/show/16dTKEEtKkIzhr1JJNMmSF?si=900902f2dca8442e) covered embeddings, model hallucinations, research papers, the job market, and much more. Join us, ask questions, and leave comments.*

## 🔧 AI Assisted Coding

### Key Takeaways for AI Engineers

- **Semantic Understanding Revolution:** There is MCP servers that use vector embeddings and dependency graphs to understand code semantically, not just syntactically  
- **Developer as Orchestrator:** The role shifts from writing code to managing AI agents that handle implementation details  
- **Embedding models to extend client :** Using simple and small embeddings models can add semantic functionality to the client side.   
-   
- **Action Items:**  
  - Experiment with semantic code search using embeddings  
  - Build or select existing MCP servers for your most common coding patterns

### What's been going on this week?

Let me start with [Guillaume's article on In-browser Semantic Search with EmbeddingGemma](https://medium.com/google-cloud/in-browser-semantic-search-with-embeddinggemma-ce37c4a1ceaa) for two reasons. First, as Guillaume admits, much of the code he proposes was written with AI assistance. While he's not a JavaScript expert, he knew exactly what he wanted to implement and, as an experienced engineer, the result was excellent. Second, it connects to [Google's release of the EmbeddingGemma](https://huggingface.co/google/embeddinggemma-300m) model, which provides an excellent tool for programmers who want to apply a fundamental part of AI differently; for semantic searches, but also for exploring semantic differences or clustering. A compact embedding model can become the Swiss Army knife of the programmer with ideas, and I invite you to try it.

The trend toward command-line AI assistance continues to strengthen with [Cursor CLI](https://cursor.com/cli), further confirming what I've mentioned repeatedly in recent weeks. This beta CLI tool promises to accelerate development directly from the terminal, representing a natural evolution as developers seek to integrate AI assistance into their existing workflows rather than switching contexts to web interfaces.

Particularly interesting is the use of AI-assisted coding to optimize AI tools themselves, as [Anthropic's research on Writing Effective Tools for Agents](https://www.anthropic.com/engineering/writing-tools-for-agents) demonstrates, where they used Claude to write tools optimized for Claude. It's a kind of meta-optimization; after all, who better than the client knows what the server should provide? Their engineers discovered that agents work better with fewer, more thoughtfully designed tools rather than wrapping every API endpoint. Tools optimized for Claude significantly outperformed human-written versions in internal tests, with agents capable of automatically improving their own tool sets through evaluation cycles. This demonstrates again how writing tools optimized for AI is different from writing API for humans, and it’s becoming more and more important.

An interesting development comes from [Faraaz Ahmad's work on making AI coding agents more efficient](https://faraazahmad.github.io/blog/blog/efficient-coding-agent/). His research addresses fundamental inefficiencies in current AI coding assistants: they're too wasteful (spending 60,000 tokens without finding all relevant functions), they don't maintain codebase context (resetting memory with each conversation), they have only surface-level understanding (relying on text-based searches), and they're susceptible to context rot. Ahmad's solution combines vector embeddings for semantic search with dependency graphs stored in Neo4j, achieving dramatic efficiency improvements. In one benchmark, semantic search reduced token usage from 47,000 to 5,000 tokens, a reduction of nearly 90%. His open-source [GraphSense tool](https://github.com/faraazahmad/graphsense) demonstrates that by applying established computer science techniques (vector embeddings for semantic similarity and graph databases for dependency tracking), we can make AI coding assistants far more practical and cost-effective.

## 🤖 Agentic AI

### Key Takeaways for AI Engineers

- **Protocol Convergence:** A2A for agent-to-agent communication and MCP for agent-to-tool interaction are becoming industry standards  
- **Extension Architecture:** A2A's extension system enables domain-specific functionality without protocol fragmentation  
- **Memory and Context Management:** Meta's REFRAG demonstrates 16x context extension with 31x faster decoding  
- **Action Items:**  
  - Evaluate A2A protocol for multi-agent system architectures  
  - Implement MCP for tool integration in current projects

### What's been going on this week?

I'll start with news about A2A, where I'm personally involved along with my team in the specification, TCK, and Java SDK development. The convergence observed toward A2A, with alternative projects like [ACP merging into the common effort](https://lfaidata.foundation/communityblog/2025/08/29/acp-joins-forces-with-a2a-under-the-linux-foundations-lf-ai-data/), demonstrates how A2A is establishing itself as the protocol for agent communication, complementing the already established MCP for agent-to-tool communication. IBM Research launched ACP in March 2025 for their BeeAI platform, but when A2A appeared a month later, both teams immediately saw alignment in their approaches and began exploring unification. Today, ACP officially merges with A2A under the Linux Foundation, with Kate Blair from IBM joining the Technical Steering Committee alongside representatives from Google, Microsoft, AWS, Cisco, Salesforce, ServiceNow, and SAP.

The [official launch of A2A protocol extensions](https://developers.googleblog.com/en/a2a-extensions-empowering-custom-agent-functionality/) shows how much we're trying to be pragmatic: while the community clearly needs a clear and stable standard, the speed of sector evolution requires a way to support extensions and experiments even at the protocol level with a different lifecycle of the base specification, but as open as it is and available to the community. Extensions are A2A's answer to this dilemma. Anyone can define, publish, and implement an extensionLogan Kilpatrick, each identified by a unique URI, making the ecosystem incredibly open and community-driven. Extensions cover diverse use cases: from the traceability extension providing deep visibility for tracking agent interactions, to domain-specific extensions like Twilio's for latency awareness in voice agent model selection, to Identity Machines' implementations of zero-trust handshakes for secure agent-to-agent calls.

Chatbots increasingly become agents themselves, as [Claude's new ability to create and edit files](https://www.anthropic.com/news/create-files) demonstratesIl mio forte sospetto è che quanto stiamo vedendo su AI Edge Gallery sia la base di quanto stanno preparando per i dispositivi home che potrebbero avvalersi di modelli locali per i compiti più semplici relativi alla voce e delegare a Gemini i compiti più complessi. Forse non al primo rilascio, ma a tendere potrebbe essere questa la strada.. Claude can now generate and modify documents, spreadsheets, presentations, and PDFs directly in the app, allowing users to transform prompts and data into downloadable files. This new capability expands Claude's utility beyond simple conversation, making it a practical tool for content creation. Similarly, users increasingly want agents, as shown by the success of [Google AI Mode](https://searchengineland.com/google-ai-mode-may-become-the-default-google-search-experience-soon-461649), which Google says will become the default search experience "soon."

The [MCP Registry launch](https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/) provides a standardized way to distribute and discover MCP servers. A community-driven project, it allows organizations to create private enterprise registries while maintaining compatibility through shared API schemas and moderation guidelines. The registry acts as an "app store" for MCP servers, facilitating discovery and implementation of new integrations.

[Raindrop AI's thoughts on Evals](https://www.raindrop.ai/blog/thoughts-on-evals) highlight that production monitoring reveals real issues that pre-deployment evaluations inevitably miss, especially when AI products become more unpredictable and personalized. Evals are collections of already-known failure cases, but agents can and often fail in ways that don't produce error codes. Agentic systems require more sophisticated evaluation approaches beyond traditional benchmarks.

Finally, [Meta's REFRAG paper](https://www.marktechpost.com/2025/09/07/meta-superintelligence-labs-introduces-refrag-scaling-rag-with-16x-longer-contexts-and-31x-faster-decoding/) is significant not just for RAG (their first case study) but also for memory and context management in agents. As I've mentioned repeatedly in recent weeks, this is where much of agent's effectiveness is decided in managing a broader context efficiently and somewhat intelligently. REFRAG extends LLM context windows by 16x and achieves up to 30.85x acceleration in time-to-first-token without compromising accuracy. The mechanism compresses retrieved passages into compact embeddings, selectively expanding only the important ones. It uses a lightweight encoder that divides retrieved passages into fixed-size chunks and compresses each into a dense embedding, with a reinforcement learning policy supervising compression to identify the most information-dense chunks.

## 🧠 AI Models and Research

### Key Takeaways for AI Engineers

- **Hallucinations as an Incentive Problem:** OpenAI research shows models hallucinate because training rewards guessing over uncertainty  
- **Determinism is Achievable:** Thinking Machines Lab proves LLM nondeterminism stems from batch invariance issues, not fundamental limitations  
- **Efficiency Through Sparsity:** Qwen3-Next's 80B model activates only 3B parameters at inference, achieving 10x throughput  
- **Action Items:**  
  - Design evaluation metrics that reward appropriate uncertainty expression  
  - Keep an eye and test open source models alternative to big tech ones

### What's been going on this week?

There's been much discussion about [OpenAI's hallucination research](https://openai.com/index/why-language-models-hallucinate/), which is important because it connects hallucinations to how models are rewarded during training. If guessing randomly is more rewarding than saying "I don't know," the model will guess. Take the example of a person's birthday: if guessing correctly is worth 10 points and both being wrong and saying "I don't know" are worth 0, it makes sense to guess randomly because you have a 1/365 chance of getting 10 points. "I don't know" guarantees zero. The solution is conceptually simple: calibrate rewards during training based on certainty, penalize confidently wrong errors more heavily, and give partial credit when models hesitate or express doubt. While conceptually simple, however, it requires revisiting how training datasets are constructed and therefore its implementation could be lengthy and costly for state-of-the-art models.

Perhaps even more interesting and formal is [Thinking Machines Lab's research](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/) providing a mathematical explanation of model nondeterminism. Their research, the first public output from Mira Murati's new lab, identifies the true cause of nondeterminism in models: contrary to common belief, it's not the non-associative nature of floating-point arithmetic combined with concurrent GPU execution, but rather the lack of batch invariance in widely used inference kernels. The primary issue is that load (and thus batch size) varies non-deterministically, causing different results even with identical inputs. The research provides PyTorch implementations of invariant operations and demonstrates Qwen3-8B running deterministically under vLLM.

Alibaba introduced [Qwen3-Max-Preview](https://venturebeat.com/ai/qwen3-max-arrives-in-preview-with-1-trillion-parameters-blazing-fast), a model with 1 trillion parameters now accessible via Qwen Chat and Alibaba Cloud. Initial benchmarks highlight improvements in instruction following, dialogue, and agentic task performance. The model supports a 262,144 token context window and is designed for complex reasoning, coding, structured data format handling like JSON, and creative tasks. [Qwen3-Next](https://qwen.ai/blog) represents a significant advancement in computational efficiency with its new architecture featuring sparse Mixture-of-Experts design with hybrid attention and multi-token prediction. Its 80B parameter base model activates only 3B parameters at inference, enabling 10x faster throughput for long-context tasks. Alibaba's models are getting closer and closer to those of American big tech companies and are currently the best open source ones available; the gap that remains to be bridged is perhaps on the context size they can handle

[GROK is working on background thinking](https://x.com/nima_owji/status/1964725529015374171), with a screenshot showing a toggle for the feature that appears to allow users to continue chatting while the assistant thinks. ByteDance's [REER (REverse-Engineered Reasoning)](https://m-a-p.ai/REER_DeepWriter/) presents an innovative paradigm that derives step-by-step reasoning from known good answers rather than building it forward through RL or imitation. Meanwhile, ByteDance's [Seedream 4.0](https://www.scmp.com/tech/big-tech/article/3325058/bytedance-unveils-new-ai-image-model-rival-google-deepminds-nano-banana) claims superiority over Google DeepMind's "Nano Banana" in prompt adherence, alignment, and aesthetics, combining features from previous versions at the same $30 per 1,000 generations cost, though these claims are based on ByteDance's internal MagicBench benchmark without an official technical report.

## 🔧 Devices and Robotics

### Key Takeaways for AI Engineers

- **Neural Coordination Breakthrough:** RoboBallet achieves multi-robot coordination using graph neural networks  
- **Brain-Computer Interfaces Advance:** Alterego's wearable interprets silent communication through facial micromovements  
- **Edge AI Acceleration:** Local translation models on mobile devices reduce cloud dependency  
- **Action Items:**  
  - Explore edge AI frameworks for on-device processing  
  - Consider multi-modal edge models for UI/UX applications

### What's been going on this week?

Watch the [RoboBallet video](https://x.com/GoogleDeepMind/status/1965040645103407572) to understand the level of multi-robot coordination achieved by neural networks. Google DeepMind's research presents an innovative approach for planning coordinated movements between multiple robots using graph neural networks and reinforcement learning. RoboBallet addresses the challenge of modern robotic production requiring collision-free coordination of multiple robots to complete numerous tasks in shared, obstacle-rich workspaces. The system uses reinforcement learning to enable scalable task and movement planning in multi-robot robotics, intelligently combining graph neural networks' capabilities to understand spatial relationships with reinforcement learning algorithms to optimize trajectories.

[Alterego's wearable](https://alterego.ai/) seems telepathic but actually exploits facial and tongue micromovements: you must speak without making sounds, moving your tongue for it to work. Still, it seems like science fiction and certainly has applications for those who can't use their hands due to other tasks or disabilities. The Boston startup and MIT Media Lab spinoff has introduced this "near-telepathic" wearable using AI to detect subtle movements and interpret a user's silent communication "at the speed of thought" without speaking aloud.

[AirPods with Apple Intelligence](https://x.com/GoogleDeepMind/status/1965040645103407572) on local iPhone and [Google's edge gallery models](https://x.com/GoogleDeepMind/status/1965040645103407572) that also add audio emphasize how small models on mobile devices can have a future for translations. The new AirPods Pro 3 include live translation enabled by Apple Intelligence, deployed locally on the iPhone.   
Google's AI Edge Gallery has added audio mode through Gemma 3n, their first multimodal model for edge devices, with the new "Audio Scribe" feature allowing completely offline transcription and translation. My strong suspicion is that what we're seeing on AI Edge Gallery is the foundation of what they're preparing for home devices that could leverage local models for simpler voice-related tasks and delegate more complex tasks to Gemini. Perhaps not at the first release, but this could be the direction they're heading.  
And probably in general for performing specific tasks or user interface functions, delegating reasoning or complex tasks to cloud models (LLMs). But already doing the first part of natural language UX or even audio UX would greatly reduce the cost of any remote LLM API calls. This seems to me the most promising direction for SLMs.

## 🏢 Business and Big Tech

### Key Takeaways for AI Engineers

- **Partnership Tensions Surface:** Microsoft diversifies from OpenAI by integrating Claude into Office 365  
- **Physical AI Investment Surge:** Robotics and autonomous systems attract massive funding rounds  
- **For-Profit Transition:** OpenAI's structural changes signal industry maturation  
- **Action Items:**  
  - Monitor multi-model strategies for enterprise deployments  
  - Track physical AI investments for emerging opportunities

### What's been going on this week?

The tensions between Microsoft and OpenAI should be emphasized, along with the fact that the [agreement they're reaching should free OpenAI to become a for-profit company](https://www.bloomberg.com/news/articles/2025-01-08/microsoft-openai-truce-clears-hurdle-in-path-to-for-profit-conversion). Microsoft and OpenAI have reached an agreement to extend their partnership, potentially facilitating OpenAI's path toward changes into a for-profit corporation. The agreement represents a victory for OpenAI, which needed Microsoft's approval for its non-profit plan before it could be formally shared with state regulators. Meanwhile, [Microsoft is lessening reliance on OpenAI by integrating Claude](https://techcrunch.com/2025/09/09/microsoft-to-lessen-reliance-on-openai-by-buying-ai-from-rival-anthropic/) into Office 365 apps, ending Microsoft's exclusive dependence on OpenAI. Beyond this, as we were saying last week, it's developing large proprietary models to break free from ChatGPT.
OpenAI has continued to stress its relationship with Microsoft by announcing competitive products, including an AI-powered LinkedIn rival and custom chips built by Broadcom.

The other two news items emphasize how investments are also moving toward so-called physical AI, namely robotics. [Amazon's Zoox launches robotaxis](https://www.cnbc.com/2025/09/10/amazons-zoox-jumps-into-us-robotaxi-race-with-las-vegas-launch-.html) on the Las Vegas Strip, offering free rides from select locations and planning to expand more widely in the city next month. Zoox becomes the first startup to offer fully autonomous rides in vehicles designed from scratch for driverless operation. [Physical Intelligence nears $5B valuation](https://www.theinformation.com/articles/physical-intelligence-nears-5b-valuation), with the San Francisco robotics AI startup reportedly close to closing a massive funding round that would value the company at $5 billion, positioning itself as a leader in robotics AI focused on systems that can physically interact with the real world.
